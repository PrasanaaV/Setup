{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, split, array_contains\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données de mapping chargées avec succès.\n",
      "Nouvelle session Spark initialisée.\n",
      "Données de Mapping : [('1', '01371'), ('2', '01372'), ('3', '01373'), ('4', '01374'), ('5', '01375'), ('6', '01376'), ('7', '01377'), ('8', '01378'), ('9', '01379'), ('10', '01380'), ('11', '01381'), ('12', '01382'), ('13', '01383'), ('14', '01384'), ('A', '01742'), ('B', '01743'), ('C', '01851'), ('D', '01852'), ('E', '01853')]\n",
      "Erreur lors de la création du DataFrame : An error occurred while calling o34.defaultParallelism.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Lire le fichier JSON pour le mapping\n",
    "try:\n",
    "    with open('./dags/files/mapping.json', 'r') as f:\n",
    "        line_mapping = json.load(f)\n",
    "    print(\"Données de mapping chargées avec succès.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la lecture du fichier JSON : {e}\")\n",
    "\n",
    "# Arrêter toute session Spark existante (cette partie peut être omise si vous redémarrez le noyau)\n",
    "if 'spark' in globals():\n",
    "    try:\n",
    "        spark.stop() # type: ignore\n",
    "        print(\"Ancienne session Spark arrêtée.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'arrêt de la session Spark : {e}\")\n",
    "\n",
    "# Initialiser une nouvelle session Spark\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.es.nodes\", \"676d19be3cd5480d980f3eb5dca1dede.us-central1.gcp.cloud.es.io\") \\\n",
    "        .config(\"spark.es.port\", \"9243\") \\\n",
    "        .config(\"spark.es.nodes.wan.only\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Nouvelle session Spark initialisée.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'initialisation de la session Spark : {e}\")\n",
    "\n",
    "# Convertir le dictionnaire en liste de tuples\n",
    "try:\n",
    "    mapping_data = [(k, v) for k, v in line_mapping.items()]\n",
    "    print(\"Données de Mapping :\", mapping_data)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la conversion des données de mappage : {e}\")\n",
    "\n",
    "# Définir le schéma\n",
    "schema = StructType([\n",
    "    StructField(\"original\", StringType(), True),\n",
    "    StructField(\"mapped\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Créer le DataFrame avec gestion des erreurs\n",
    "try:\n",
    "    mapping_df = spark.createDataFrame(mapping_data, schema)\n",
    "    mapping_df.show()\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la création du DataFrame : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001A0E7E47260>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
